{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Download data\n",
    "print(\"Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"sidharth178/car-prices-dataset\")\n",
    "print(f\"Dataset downloaded to {path}\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(path + \"/train.csv\")\n",
    "print(f\"Loaded dataset with {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nChecking for missing values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0] if any(missing_values > 0) else \"No missing values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"Number of vehicles: {len(df)}\")\n",
    "print(f\"Number of unique manufacturers: {df['Manufacturer'].nunique()}\")\n",
    "print(f\"Number of unique models: {df['Model'].nunique()}\")\n",
    "print(f\"Vehicle production years: from {df['Prod. year'].min()} to {df['Prod. year'].max()}\")\n",
    "print(f\"Price range: ${df['Price'].min()} to ${df['Price'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning function for car price dataset\n",
    "    \"\"\"\n",
    "    print(\"\\nCleaning data...\")\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Fix data types and formats\n",
    "    print(\"Fixing data types...\")\n",
    "    # Fix Engine volume (separate Turbo indicator)\n",
    "    df_clean['Turbo'] = df_clean['Engine volume'].str.contains('Turbo').astype(int)\n",
    "    df_clean['Engine volume'] = df_clean['Engine volume'].str.replace(' Turbo', '').astype(float)\n",
    "    \n",
    "    # Fix mileage\n",
    "    df_clean['Mileage'] = df_clean['Mileage'].str.replace(' km', '').astype(float)\n",
    "    \n",
    "    # Fix categorical variables\n",
    "    df_clean['Leather interior'] = df_clean['Leather interior'].map({'Yes': 1, 'No': 0})\n",
    "    df_clean['Wheel'] = df_clean['Wheel'].map({'Right wheel': 1, 'Left wheel': 0})\n",
    "    \n",
    "    # 2. Clean problematic columns\n",
    "    print(\"Cleaning problematic columns...\")\n",
    "    # Clean doors - extract first number and standardize format\n",
    "    df_clean['Doors'] = (\n",
    "        df_clean['Doors']\n",
    "        .astype(str)\n",
    "        .str.extract('(\\d+)', expand=False)  # Extract first numeric sequence\n",
    "        .fillna('4')                         # Fill missing with most common value\n",
    "        .astype(int)                         # Convert to integer to remove leading zeros\n",
    "        .astype(str)                         # Convert back to string for categorical handling\n",
    "    )\n",
    "    \n",
    "    # Clean levy - handle '-' and convert to numeric\n",
    "    df_clean['Levy'] = (\n",
    "        pd.to_numeric(df_clean['Levy'].replace('-', '0'), errors='coerce')\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    # 3. Remove outliers and invalid entries\n",
    "    print(\"Removing outliers and invalid entries...\")\n",
    "    # Filter unrealistic prices\n",
    "    min_price = 1000  # Minimum realistic price\n",
    "    df_clean = df_clean[df_clean['Price'] >= min_price]\n",
    "    \n",
    "    # Identify new cars (for mileage check)\n",
    "    current_year = 2024\n",
    "    df_clean['is_new'] = (current_year - df_clean['Prod. year']) <= 1\n",
    "    \n",
    "    # Handle suspicious mileage values\n",
    "    condition = (\n",
    "        (df_clean['Mileage'] > 0) | \n",
    "        ((df_clean['Mileage'] == 0) & (df_clean['is_new']))\n",
    "    )\n",
    "    df_clean = df_clean[condition]\n",
    "    \n",
    "    # Remove extreme prices using IQR\n",
    "    Q1 = df_clean['Price'].quantile(0.10)\n",
    "    Q3 = df_clean['Price'].quantile(0.90)\n",
    "    IQR = Q3 - Q1\n",
    "    max_price = Q3 + 1.5 * IQR\n",
    "    df_clean = df_clean[df_clean['Price'] <= max_price]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    original_count = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"Removed {original_count - len(df_clean)} duplicates.\")\n",
    "    \n",
    "    # 4. Feature engineering\n",
    "    print(\"Creating new features...\")\n",
    "    # Add car age\n",
    "    df_clean['Car_Age'] = current_year - df_clean['Prod. year']\n",
    "    \n",
    "    # Add engine related features\n",
    "    df_clean['Engine_Age_Ratio'] = df_clean['Engine volume'] / df_clean['Car_Age'].clip(lower=1)\n",
    "    df_clean['Cylinders_per_liter'] = df_clean['Cylinders'] / df_clean['Engine volume'].clip(lower=0.1)\n",
    "    \n",
    "    # 5. Handle high-cardinality categories\n",
    "    print(\"Handling high-cardinality categories...\")\n",
    "    # Handle rare car models\n",
    "    model_counts = df_clean['Model'].value_counts()\n",
    "    rare_threshold = 10\n",
    "    mask = df_clean['Model'].isin(model_counts[model_counts < rare_threshold].index)\n",
    "    df_clean.loc[mask, 'Model'] = df_clean.loc[mask].apply(\n",
    "        lambda x: f\"Other_{x['Manufacturer']}\", axis=1)\n",
    "    \n",
    "    # Remove temporary columns\n",
    "    df_clean = df_clean.drop('is_new', axis=1)\n",
    "    \n",
    "    print(f\"Data cleaning completed. Remaining rows: {len(df_clean)}\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "df_cleaned = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define allowed values (should match those in app.py)\n",
    "VALID_DOORS = [\"2\", \"4\", \"5\", \"6\"]\n",
    "VALID_FUEL_TYPES = [\"Petrol\", \"Diesel\", \"CNG\", \"LPG\"]\n",
    "VALID_WHEEL_VALUES = [0, 1]  # 0=Left wheel, 1=Right wheel\n",
    "\n",
    "# Validate cleaned data\n",
    "print(\"\\nValidating cleaned data...\")\n",
    "for col, allowed in [\n",
    "    ('Doors', VALID_DOORS),\n",
    "    ('Fuel type', VALID_FUEL_TYPES),\n",
    "    ('Wheel', VALID_WHEEL_VALUES)\n",
    "]:\n",
    "    # Find invalid values\n",
    "    invalid_mask = ~df_cleaned[col].isin(allowed)\n",
    "    invalid_count = invalid_mask.sum()\n",
    "    \n",
    "    if invalid_count > 0:\n",
    "        print(f\"Found {invalid_count} invalid entries in {col}:\")\n",
    "        print(df_cleaned[invalid_mask][col].unique())\n",
    "        \n",
    "        # Remove invalid entries\n",
    "        df_cleaned = df_cleaned[~invalid_mask]\n",
    "        print(f\"Removed {invalid_count} invalid entries from {col}\")\n",
    "    else:\n",
    "        print(f\"{col} values all valid\")\n",
    "\n",
    "print(f\"\\nFinal dataset size after validation: {len(df_cleaned)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_cleaned['Price'], kde=True, bins=50)\n",
    "plt.title('Price Distribution After Cleaning')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize relationships between key features and price\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Car age vs. Price\n",
    "sns.scatterplot(x='Car_Age', y='Price', data=df_cleaned, alpha=0.5, ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Car Age vs. Price')\n",
    "\n",
    "# Mileage vs. Price\n",
    "sns.scatterplot(x='Mileage', y='Price', data=df_cleaned, alpha=0.5, ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Mileage vs. Price')\n",
    "\n",
    "# Engine volume vs. Price\n",
    "sns.scatterplot(x='Engine volume', y='Price', data=df_cleaned, alpha=0.5, ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Engine Volume vs. Price')\n",
    "\n",
    "# Top manufacturers by average price\n",
    "top_manufacturers = df_cleaned.groupby('Manufacturer')['Price'].mean().nlargest(10)\n",
    "sns.barplot(x=top_manufacturers.index, y=top_manufacturers.values, ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Top 10 Manufacturers by Average Price')\n",
    "axs[1, 1].set_xticklabels(axs[1, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuel type distribution and average price by fuel type\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Fuel type distribution\n",
    "fuel_counts = df_cleaned['Fuel type'].value_counts()\n",
    "sns.barplot(x=fuel_counts.index, y=fuel_counts.values, ax=ax1)\n",
    "ax1.set_title('Distribution of Fuel Types')\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Average price by fuel type\n",
    "avg_price_by_fuel = df_cleaned.groupby('Fuel type')['Price'].mean().sort_values(ascending=False)\n",
    "sns.barplot(x=avg_price_by_fuel.index, y=avg_price_by_fuel.values, ax=ax2)\n",
    "ax2.set_title('Average Price by Fuel Type')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "ax2.set_ylabel('Average Price ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Category', y='Price', data=df_cleaned, order=df_cleaned.groupby('Category')['Price'].median().sort_values(ascending=False).index)\n",
    "plt.title('Price Distribution by Vehicle Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPreparing features and target...\")\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = ['Prod. year', 'Engine volume', 'Mileage', 'Cylinders', 'Airbags',\n",
    "                   'Car_Age', 'Engine_Age_Ratio', 'Cylinders_per_liter']\n",
    "categorical_features = ['Manufacturer', 'Model', 'Category', 'Fuel type', \n",
    "                      'Gear box type', 'Drive wheels', 'Color', 'Levy', 'Doors']\n",
    "binary_features = ['Leather interior', 'Wheel', 'Turbo']\n",
    "\n",
    "# Drop ID column if it exists\n",
    "if 'ID' in df_cleaned.columns:\n",
    "    df_cleaned = df_cleaned.drop('ID', axis=1)\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(\"\\nChecking for missing values after cleaning:\")\n",
    "missing_values = df_cleaned.isnull().sum()\n",
    "print(missing_values[missing_values > 0] if any(missing_values > 0) else \"No missing values found.\")\n",
    "\n",
    "# Fix any remaining missing values\n",
    "for col in df_cleaned.columns:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        if col in numeric_features:\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "        else:\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])\n",
    "\n",
    "# Separate features and target\n",
    "X = df_cleaned.drop('Price', axis=1)\n",
    "y = df_cleaned['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting data into train, validation, and test sets...\")\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating preprocessing pipeline...\")\n",
    "\n",
    "# Define transformers with imputers to handle any missing values\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Column transformer that applies the right transformation to each column type\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('bin', 'passthrough', binary_features)\n",
    "    ])\n",
    "\n",
    "# Initialize and fit the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Fit preprocessing pipeline on training data\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_val_processed = preprocessing_pipeline.transform(X_val)\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "print(f\"Processed feature dimensions: {X_train_processed.shape[1]} features\")\n",
    "\n",
    "# Check for NaN values in processed data\n",
    "print(\"\\nChecking for NaN values in processed data:\")\n",
    "nan_count = np.isnan(X_train_processed).sum()\n",
    "print(f\"Found {nan_count} NaN values in processed training data\")\n",
    "\n",
    "# Handle any NaN values if present\n",
    "if nan_count > 0:\n",
    "    print(\"Handling remaining NaN values by imputation\")\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_processed = imputer.fit_transform(X_train_processed)\n",
    "    X_val_processed = imputer.transform(X_val_processed)\n",
    "    X_test_processed = imputer.transform(X_test_processed)\n",
    "    \n",
    "    # Verify NaN values are gone\n",
    "    print(f\"After imputation: {np.isnan(X_train_processed).sum()} NaN values remain\")\n",
    "    with open('model_artifacts/imputer.pkl', 'wb') as f:\n",
    "        pickle.dump(imputer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPerforming feature selection...\")\n",
    "selector = SelectKBest(f_regression, k='all')\n",
    "selector.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame({\n",
    "    'Score': selector.scores_,\n",
    "})\n",
    "\n",
    "# Sort features by score\n",
    "scores_sorted = scores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "# Select top features (you can adjust k)\n",
    "k = min(50, X_train_processed.shape[1])  # Select top 50 features or all if less than 50\n",
    "print(f\"Selecting top {k} features based on F-test...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_selector = SelectKBest(f_regression, k=k)\n",
    "X_train_selected = final_selector.fit_transform(X_train_processed, y_train)\n",
    "X_val_selected = final_selector.transform(X_val_processed)\n",
    "X_test_selected = final_selector.transform(X_test_processed)\n",
    "print(f\"Final feature dimensions after selection: {X_train_selected.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('model_artifacts', exist_ok=True)\n",
    "\n",
    "# Save preprocessed data\n",
    "print(\"\\nSaving preprocessed data...\")\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train_selected,\n",
    "    'X_val': X_val_selected,\n",
    "    'X_test': X_test_selected,\n",
    "    'y_train': y_train.values,\n",
    "    'y_val': y_val.values,\n",
    "    'y_test': y_test.values\n",
    "}\n",
    "\n",
    "with open('model_artifacts/preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "# Save the preprocessing pipeline with pickle\n",
    "with open('model_artifacts/preprocessing_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_pipeline, f)\n",
    "\n",
    "# Save the feature selector with pickle\n",
    "with open('model_artifacts/feature_selector.pkl', 'wb') as f:\n",
    "    pickle.dump(final_selector, f)\n",
    "\n",
    "# Save column information for future reference\n",
    "column_info = {\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'binary_features': binary_features,\n",
    "    'all_features': X_train.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('model_artifacts/column_info.pkl', 'wb') as f:\n",
    "    pickle.dump(column_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPreprocessing completed and artifacts saved!\")\n",
    "print(\"\"\"\n",
    "The following files have been created:\n",
    "- model_artifacts/preprocessed_data.pkl: Contains the processed train/val/test data\n",
    "- model_artifacts/preprocessing_pipeline.pkl: Preprocessing pipeline for new data\n",
    "- model_artifacts/feature_selector.pkl: Feature selector for dimensionality reduction\n",
    "- model_artifacts/column_info.pkl: Information about feature types and names\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function to demonstrate how to use the saved artifacts for new data\n",
    "def preprocess_new_data(new_data):\n",
    "    \"\"\"\n",
    "    Example function demonstrating how to preprocess new data using saved artifacts\n",
    "    \"\"\"\n",
    "    # Load preprocessing pipeline and feature selector\n",
    "    with open('model_artifacts/preprocessing_pipeline.pkl', 'rb') as f:\n",
    "        preprocessing_pipeline = pickle.load(f)\n",
    "    \n",
    "    with open('model_artifacts/feature_selector.pkl', 'rb') as f:\n",
    "        feature_selector = pickle.load(f)\n",
    "    \n",
    "    # Preprocess the new data\n",
    "    X_processed = preprocessing_pipeline.transform(new_data)\n",
    "    \n",
    "    # Handle any NaN values if imputation was needed\n",
    "    if os.path.exists('model_artifacts/imputer.pkl'):\n",
    "        with open('model_artifacts/imputer.pkl', 'rb') as f:\n",
    "            imputer = pickle.load(f)\n",
    "        X_processed = imputer.transform(X_processed)\n",
    "    \n",
    "    # Apply feature selection\n",
    "    X_selected = feature_selector.transform(X_processed)\n",
    "    \n",
    "    return X_selected\n",
    "\n",
    "print(\"\\nReady for model building and comparison!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EXAMPLE 1 ---\n",
      "{\n",
      "  \"prod_year\": 2016,\n",
      "  \"mileage\": 215572.0,\n",
      "  \"manufacturer\": \"HYUNDAI\",\n",
      "  \"model\": \"Sonata\",\n",
      "  \"engine_volume\": 2.0,\n",
      "  \"cylinders\": 4,\n",
      "  \"fuel_type\": \"LPG\",\n",
      "  \"gear_box_type\": \"Automatic\",\n",
      "  \"drive_wheels\": \"Front\",\n",
      "  \"category\": \"Sedan\",\n",
      "  \"leather_interior\": true,\n",
      "  \"color\": \"Silver\",\n",
      "  \"airbags\": 4,\n",
      "  \"turbo\": false,\n",
      "  \"levy\": 891.0,\n",
      "  \"doors\": \"4\",\n",
      "  \"wheel\": \"Left\"\n",
      "}\n",
      "\n",
      "--- EXAMPLE 2 ---\n",
      "{\n",
      "  \"prod_year\": 2014,\n",
      "  \"mileage\": 85500.0,\n",
      "  \"manufacturer\": \"TOYOTA\",\n",
      "  \"model\": \"Corolla\",\n",
      "  \"engine_volume\": 1.8,\n",
      "  \"cylinders\": 4,\n",
      "  \"fuel_type\": \"Petrol\",\n",
      "  \"gear_box_type\": \"Tiptronic\",\n",
      "  \"drive_wheels\": \"Front\",\n",
      "  \"category\": \"Sedan\",\n",
      "  \"leather_interior\": true,\n",
      "  \"color\": \"Red\",\n",
      "  \"airbags\": 12,\n",
      "  \"turbo\": false,\n",
      "  \"levy\": 584.0,\n",
      "  \"doors\": \"4\",\n",
      "  \"wheel\": \"Left\"\n",
      "}\n",
      "\n",
      "--- EXAMPLE 3 ---\n",
      "{\n",
      "  \"prod_year\": 2009,\n",
      "  \"mileage\": 109139.0,\n",
      "  \"manufacturer\": \"MERCEDES-BENZ\",\n",
      "  \"model\": \"E 350\",\n",
      "  \"engine_volume\": 3.5,\n",
      "  \"cylinders\": 6,\n",
      "  \"fuel_type\": \"Diesel\",\n",
      "  \"gear_box_type\": \"Automatic\",\n",
      "  \"drive_wheels\": \"Rear\",\n",
      "  \"category\": \"Sedan\",\n",
      "  \"leather_interior\": true,\n",
      "  \"color\": \"Black\",\n",
      "  \"airbags\": 12,\n",
      "  \"turbo\": false,\n",
      "  \"levy\": 1624.0,\n",
      "  \"doors\": \"4\",\n",
      "  \"wheel\": \"Left\"\n",
      "}\n",
      "\n",
      "--- EXAMPLE 4 ---\n",
      "{\n",
      "  \"prod_year\": 2012,\n",
      "  \"mileage\": 162911.0,\n",
      "  \"manufacturer\": \"HYUNDAI\",\n",
      "  \"model\": \"Elantra\",\n",
      "  \"engine_volume\": 1.6,\n",
      "  \"cylinders\": 4,\n",
      "  \"fuel_type\": \"Petrol\",\n",
      "  \"gear_box_type\": \"Automatic\",\n",
      "  \"drive_wheels\": \"Front\",\n",
      "  \"category\": \"Sedan\",\n",
      "  \"leather_interior\": true,\n",
      "  \"color\": \"White\",\n",
      "  \"airbags\": 4,\n",
      "  \"turbo\": false,\n",
      "  \"levy\": 531.0,\n",
      "  \"doors\": \"4\",\n",
      "  \"wheel\": \"Left\"\n",
      "}\n",
      "\n",
      "--- EXAMPLE 5 ---\n",
      "{\n",
      "  \"prod_year\": 2012,\n",
      "  \"mileage\": 148310.0,\n",
      "  \"manufacturer\": \"HYUNDAI\",\n",
      "  \"model\": \"Santa FE\",\n",
      "  \"engine_volume\": 2.0,\n",
      "  \"cylinders\": 4,\n",
      "  \"fuel_type\": \"Diesel\",\n",
      "  \"gear_box_type\": \"Automatic\",\n",
      "  \"drive_wheels\": \"Front\",\n",
      "  \"category\": \"Jeep\",\n",
      "  \"leather_interior\": true,\n",
      "  \"color\": \"Silver\",\n",
      "  \"airbags\": 4,\n",
      "  \"turbo\": false,\n",
      "  \"levy\": 642.0,\n",
      "  \"doors\": \"4\",\n",
      "  \"wheel\": \"Left\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1) Sample a few rows\n",
    "sample_size = 5\n",
    "sample_rows = df_cleaned.sample(sample_size, random_state=42)\n",
    "\n",
    "# 2) Convert each row to JSON\n",
    "json_examples = []\n",
    "for idx, row in sample_rows.iterrows():\n",
    "    # Convert numeric wheel -> \"Left\"/\"Right\"\n",
    "    wheel_str = \"Left\" if row[\"Wheel\"] == 0 else \"Right\"\n",
    "    \n",
    "    # Force Python bool\n",
    "    interior_bool = bool(row[\"Leather interior\"] == 1)\n",
    "    turbo_bool = bool(row[\"Turbo\"] == 1)\n",
    "    \n",
    "    # Handle '-' for Levy\n",
    "    levy_val = row[\"Levy\"]\n",
    "    if levy_val == '-':\n",
    "        levy_val = '0'\n",
    "    \n",
    "    car_dict = {\n",
    "        \"prod_year\": int(row[\"Prod. year\"]),\n",
    "        \"mileage\": float(row[\"Mileage\"]),\n",
    "        \"manufacturer\": str(row[\"Manufacturer\"]),\n",
    "        \"model\": str(row[\"Model\"]),\n",
    "        \"engine_volume\": float(row[\"Engine volume\"]),\n",
    "        \"cylinders\": int(row[\"Cylinders\"]),\n",
    "        \"fuel_type\": str(row[\"Fuel type\"]),\n",
    "        \"gear_box_type\": str(row[\"Gear box type\"]),\n",
    "        \"drive_wheels\": str(row[\"Drive wheels\"]),\n",
    "        \"category\": str(row[\"Category\"]),\n",
    "        \"leather_interior\": interior_bool,     # Python bool\n",
    "        \"color\": str(row[\"Color\"]),\n",
    "        \"airbags\": int(row[\"Airbags\"]),\n",
    "        \"turbo\": turbo_bool,                   # Python bool\n",
    "        \"levy\": float(levy_val),\n",
    "        \"doors\": str(row[\"Doors\"]),\n",
    "        \"wheel\": wheel_str\n",
    "    }\n",
    "\n",
    "    json_examples.append(car_dict)\n",
    "\n",
    "# 3) Print JSON\n",
    "for i, example in enumerate(json_examples, start=1):\n",
    "    print(f\"--- EXAMPLE {i} ---\")\n",
    "    print(json.dumps(example, indent=2))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
